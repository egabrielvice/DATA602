# -*- coding: utf-8 -*-
"""assignment_07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fw8fLOQf271X8-mzkRdPu7cxWKT74Zp4

# **Assignment 7**
# **Course**: DATA 602

# **Name**: Escarlet Gabriel Vicente
# **Weeks 8 & 9 - Pandas**

#Introduction


For this assignment, an eCommerce transaction dataset from Kaggle was selected.

**Dataset source**: https://www.kaggle.com/datasets/carrie1/ecommerce-data

This dataset contains transactional records from an online retail store, including invoice numbers, product identifiers, quantities, unit prices, transaction dates, customer identifiers, and country information.

The dataset was chosen because it represents real-world business data and contains multiple data quality issues, such as missing values, duplicate records, inconsistent data types, invalid values, and character encoding challenges. These characteristics make it well suited for demonstrating data exploration and data wrangling using Pandas.

______________
# Data Exploration
In order to understand the structure, content, and overall quality of the dataset before applying any transformations, an initial data exploration is conducted.

### 1. Load Libraries
The analysis begins by loading the required Python libraries. Pandas serves as the
primary tool for data manipulation, while NumPy supports numerical operations.
Visualization libraries are included to allow graphical exploration when appropriate.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### 2. Load Data
The dataset is imported into a Pandas DataFrame to enable efficient exploration and analysis. A predefined character encoding is applied during ingestion to resolve encoding discrepancies and ensure accurate data processing.

An initial preview of the dataset is displayed to gain a general understanding of the variables, data formats, and overall structure. This step provides early insight into column names, value ranges, and potential inconsistencies before deeper inspection.
"""

df = pd.read_csv("/data.csv", encoding="latin1")
df.head(10)

"""### 3. Structure & Info

The structure of the dataset is examined using the `info()` method, which provides information on the number of observations, variable names, data types, and non-null counts. This step is essential for identifying improperly coded variables, such as dates stored as strings or numeric identifiers stored as floating-point values.

Additionally, non-null counts reveal the presence and extent of missing values across columns, informing later decisions regarding data cleaning and imputation.
"""

df.info()

"""### 4. Summary Statistics

Summary statistics are computed to examine the central tendency and dispersion of numeric variables. Measures such as the mean, median, minimum, maximum, and quartiles provide insight into typical values, variability, and the potential presence of outliers.

This step helps assess whether numeric values fall within reasonable ranges and highlights anomalies that may require further investigation.
"""

df.describe()

"""### 5. Missing Values

A dedicated check for missing values is performed to quantify the number of null entries in each column. Identifying missing data is essential, as unaddressed null values can bias results or cause errors in calculations and aggregations.

Understanding where and how frequently missing values occur informs the choice of appropriate handling strategies, such as imputation, labeling, or row removal.
"""

df.isna().sum().sort_values(ascending=False)

"""### 6. Duplicated Rows

Duplicate rows are identified to determine whether repeated records exist in the dataset. Duplicate entries can distort summary statistics and aggregated results if not handled appropriately.

Detecting duplicates at this stage ensures that subsequent analyses are based on unique and accurate observations.
"""

df.duplicated().sum()

"""# Data Exploration Findings

The exploratory analysis reveals several data quality issues that must be addressed before reliable analysis can be conducted:


*   Missing values are present in customer identifiers and product descriptions.
*   Some variables, such as invoice dates, are stored as strings instead of datetime objects.
*   The dataset contains duplicate records.
*   Invalid values, including negative quantities, are present.
*   The dataset requires special handling due to non-UTF-8 character encoding.

These findings confirm that systematic data wrangling is necessary prior to further analysis.

# Data Wrangling

In order to prepare the dataset for meaningful analysis, a series of data wrangling
steps are applied. These steps address data quality issues identified during the
exploration phase, including inconsistent formatting, missing values, invalid entries,
and redundant information. Each subsection corresponds to a specific transformation
performed on the dataset.

### 1. Modify Multiple Column Names

Column names are standardized by converting them to lowercase and replacing spaces
with underscores. This improves readability, ensures consistency, and aligns with
common Python and Pandas naming conventions.
"""

df = df.rename(columns={
    "InvoiceNo": "invoice_no",
    "StockCode": "stock_code",
    "Description": "description",
    "Quantity": "quantity",
    "InvoiceDate": "invoice_date",
    "UnitPrice": "unit_price",
    "CustomerID": "customer_id",
    "Country": "country"
})
df.head()

"""### 2. Review and Correct Data Types

The structure of the dataset is examined to identify variables that are improperly
coded. In particular, date variables stored as strings are converted to datetime
objects to enable accurate time-based analysis.
"""

df["invoice_date"] = pd.to_datetime(df["invoice_date"], errors="coerce")
df.info()

"""### 3. Fix Missing and Invalid Values

Missing values are present in several columns, most notably in customer identifiers.
These missing values likely represent anonymous or guest transactions and are handled
in a way that preserves the transaction records while clearly distinguishing missing
identifiers.
"""

df["customer_id"].isna().sum()

df["customer_id"] = df["customer_id"].fillna(-1)
df["customer_id"].isna().sum()

"""### 4. Create New Columns from Existing Data

A new column representing the total transaction value is created by multiplying the
quantity of items purchased by the unit price. This derived variable provides a more
meaningful measure for financial analysis.
"""

df.columns

df["total_price"] = df["quantity"] * df["unit_price"]
df.columns

"""### 5. Drop Unnecessary Columns

Columns that are not required for the analytical objectives are removed to reduce
noise and simplify the dataset.
"""

df = df.drop(columns=["description"])
df.columns

"""### 6. Drop Invalid Rows

Rows containing invalid values are removed from the dataset. Specifically,
transactions with non-positive quantities are excluded, as these values represent
returns or data entry errors that could distort summary statistics.
"""

df.shape

df = df[df["quantity"] > 0]
df.shape

"""### 7. Sort the Dataset by Multiple Variables

The dataset is sorted using multiple variables to improve interpretability.
Sorting by country and transaction value allows for easier identification of
high-value transactions within each group.
"""

df.sort_values(by=["country", "total_price"], ascending=[True, False]).head()

"""### 8. Filter the Dataset Based on a Condition

A subset of the dataset is created by filtering transactions that exceed a specified
transaction value threshold. This enables focused analysis of higher-value purchases.
"""

high_value = df[df["total_price"] > 500]
high_value.head()

high_value.shape

"""### 9. Standardize String Values

Text values in the country column are converted to a consistent case to prevent
grouping inconsistencies caused by variations in capitalization.
"""

df["country"] = df["country"].str.upper()
df["country"].value_counts().head()

"""### 10. Validate Numeric Values in a Column

A validation check is performed to determine whether invoice numbers contain only
numeric characters. This helps identify mixed or improperly formatted identifiers.
"""

df["invoice_no"].str.isnumeric().value_counts()

"""### 11. Group the Dataset by One Variable

The dataset is grouped by country, and summary statistics including the mean,
minimum, and maximum transaction values are calculated. This provides insight into
how transaction values vary across regions.
"""

df.groupby("country")["total_price"].agg(["mean", "min", "max"]).head()

"""### 12. Group the Dataset by Two Variables and Sort Results

The dataset is grouped by both country and product code, and the aggregated results
are sorted to highlight the most significant combinations across multiple dimensions.

"""

df.groupby(["country", "stock_code"])["total_price"] \
  .mean() \
  .sort_values(ascending=False) \
  .head(10)

"""# Conclusions  

This analysis demonstrated the importance of systematic data exploration and wrangling when working with real-world transactional data. Several key insights emerged from the cleaned dataset:

* The dataset contained substantial data quality issues, including missing customer identifiers, improperly formatted date variables, duplicate records, and invalid quantity values. Addressing these issues was necessary to ensure accurate analysis.
* Creating a derived transaction-level metric (`total_price`) enabled more meaningful financial analysis and supported effective filtering, sorting, and aggregation.
* Transaction values varied significantly across countries and products, with a small subset of high-value transactions accounting for a disproportionate share of total revenue.
* Grouping by multiple dimensions (country and product) revealed that certain productâ€“country combinations consistently generate higher transaction values.

Overall, this assignment highlights how uncleaned data can distort results and how Pandas provides efficient tools to transform raw data into an analysis-ready format.

With additional time, further analysis could include:

* Visualizations to explore sales trends over time
* Deeper investigation into customer purchasing behavior
* Integration of external datasets to enrich geographic or product-level insights
* Time-series or predictive modeling to forecast future sales patterns
"""